---
title: Algorithmic fairness
author: Koen Derks
output: 
  html_document:
    toc: true
    toc_depth: 3
bibliography: references.bib
csl: apa.csl
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(jfa)
```

## Introduction

Welcome to the 'Algorithmic fairness' vignette of the **jfa** package. This page
provides a comprehensive example of how to use the `model_fairness()` and the `fairness_selection()` functions in the package. 

## Function: `model_fairness()`

The `model_fairness()` function offers methods to evaluate fairness in
algorithmic decision-making systems. It computes various model-agnostic metrics
based on the observed and predicted labels in a dataset. The fairness metrics
that can be calculated include demographic parity, proportional parity,
predictive rate parity, accuracy parity, false negative rate parity, false
positive rate parity, true positive rate parity, negative predicted value
parity, and specificity parity [@calders_2010; @chouldechova_2017;
@feldman_2015; @zafar_2017; @friedler_2019]. Furthermore, the metrics are tested
for equality between protected groups in the data.

*Practical example:*

To demonstrate the usage of the`model_fairness()` and the `fairness_selection()`
functions, we will use the well-known COMPAS dataset. The COMPAS 
(Correctional Offender Management Profiling for Alternative Sanctions) software 
is a case management and decision support tool employed by certain U.S. courts 
to evaluate the likelihood of a defendant becoming a recidivist (repeat offender).

The `compas` data, which is included in the package, contains predictions made
by the COMPAS algorithm for various defendants. The data can be loaded using
`data("compas")` and includes information for each defendant, such as whether
the defendant committed a crime within two years following the court case
(`TwoYrRecidivism`), personal characteristics like gender and ethnicity, and
whether the software predicted the defendant to be a recidivist (`Predicted`).

```{r}
data("compas")
head(compas)
```

We will examine whether the COMPAS algorithm demonstrates fairness with respect
to the sensitive attribute `Ethnicity`. In this context, a positive prediction
implies that a defendant is classified as a reoffender, while a negative
prediction implies that a defendant is classified as a non-reoffender. The
fairness metrics provide insights into whether there are disparities in the
predictions of the algorithm for different ethnic groups. By calculating and
reviewing these metrics, we can determine whether the algorithm displays any
discriminatory behavior towards specific ethnic groups. If significant
disparities exist, further investigation may be necessary, and potential
modifications to the algorithm may be required to ensure fairness in its
predictions.

Before we begin, let's briefly explain the basis of all fairness metrics: the
confusion matrix. This matrix compares observed versus predicted labels,
highlighting the algorithm's prediction mistakes. The confusion matrix consists
of true positives (TP), false positives (FP), true negatives (TN), and false
negatives (FN). The confusion matrix for the `African_American` group is shown
below. For instance, there are 629 individuals in this group who are incorrectly
predicted to be reoffenders, representing a false positive in this confusion
matrix.

|                           | `Predicted` = `no` | `Predicted` = `yes`   |
| ------------------------: | :----------------: | :-------------------: |
| `TwoYrRecidivism` = `no`  | 885 (`TN`)         | 629 (`FP`)            |
| `TwoYrRecidivism` = `yes` | 411 (`FN`)         | 1250 (`TP`)           |

To demonstrate the usage of the `model_fairness()` function, let's interpret the
complete set of fairness metrics for the African American, Asian, and Hispanic
groups, comparing them to the privileged group (Caucasian). For a more detailed
explanation of some of these metrics, we refer to @pessach_2022. However, it is
important to note that not all fairness measures are equally suitable for all
audit situations. 

<p align='center'><img src='fairness-tree.png' alt='fairness' width='90%'></p>

1. **Demographic parity (Statistical parity)**: Compares the number of positive
  predictions (i.e., reoffenders) between each unprivileged (i.e., ethnic) group
  and the privileged group. Note that, since demographic parity is not a
  proportion, statistical inference about its equality to the privileged group
  is not supported.

    The formula for the number of positive predictions is $P = TP + FP$, and
      the demographic parity for unprivileged group $i$ is given by
      $DP = \frac{P_{i}}{P_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "dp"
)
    ```

    ***Interpretation:***
    - *African American*: The demographic parity for African Americans compared
      to Caucasians is 2.7961, indicating that for these data there are nearly
      three times more African Americans predicted as reoffenders by the
      algorithm than Caucasians.
    - *Asian*: The demographic parity for Asians is very close to zero
      (0.0059524), indicating that there are many less Asians (4) that are
      predicted as reoffenders in these data than there are Caucasians (672).
      Naturally, this can be explained because of the lack of Asian people (31)
      in the data.
    - *Hispanic*: The demographic parity for Hispanics is 0.22173, meaning that
      there are about five times less Hispanics predicted as reoffenders in
      these data than that there are Caucasians.

2. **Proportional parity (Disparate impact)**: Compares the proportion of
  positive predictions of each unprivileged group to that in the privileged
  group. For example, in the case that a positive prediction represents a
  reoffender, proportional parity requires the proportion of predicted
  reoffenders to be similar across ethnic groups.

    The formula for the proportion of positive predictions is 
      $PP = \frac{TP + FP}{TP + FP + TN + FN}$, and the proportional parity
      for unprivileged group $i$ is given by $\frac{PP_{i}}{PP_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "pp"
)
    ```

    ***Interpretation:***
    - *African American*: The proportional parity for African Americans compared
      to Caucasians is 1.8521. This indicates that African Americans are
      approximately 1.85 times more likely to get a positive prediction than
      Caucasians. Again, this suggests potential bias in the algorithm's
      predictions against African Americans. The p-value is smaller than .05,
      indicating that the null hypothesis of proportional parity should be
      rejected [@fisher_1970].
    - *Asian*: The proportional parity for Asians is 0.4038, indicating that
      their positive prediction rate is lower than for Caucasians. This may
      suggest potential underestimation of reoffenders among Asians.
    - *Hispanic*: The proportional parity for Hispanics is 0.91609, suggesting
      that their positive prediction rate is close to the privileged group.
      This indicates relatively fair treatment of Hispanics in the algorithm's
      predictions.

    This is a good time to show the `summary()` and `plot()` functions
      associated with the `model_fairness()` function. Let's examine the
      previous function call again, but instead of printing the output to the
      console, this time we store the output in `x` and run the `summary()` and
      `plot()` functions on this object.

    ```{r, fig.align="center", fig.height=4, fig.width=6}
x <- model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "pp"
)
summary(x)
plot(x, type = "estimates")
    ```

3. **Predictive rate parity (Equalized odds)**: Compares the overall positive
  prediction rates (e.g., the precision) of each unprivileged group to the
  privileged group.

    The formula for the precision is $PR = \frac{TP}{TP + FP}$, and the
      predictive rate parity for unprivileged group $i$ is given by
      $PRP = \frac{PR_{i}}{PR_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "prp"
)
    ```

    ***Interpretation:***
    - *African American*: The predictive rate parity for African Americans
      is 1.1522. This suggests that the precision for African Americans is
      approximately 1.15 times higher than for Caucasians, which indicates
      potential 'favoritism' towards African Americans in the overall positive
      predictions made by the algorithm.
    - *Asian*: The predictive rate parity for Asians is 0.86598, indicating that
      their precision is lower than for Caucasians. This suggests potential
      underestimation of reoffenders among Asians by the algorithm.
    - *Hispanic*: The predictive rate parity for Hispanics is 1.0229, suggesting
      their overall positive prediction rate is very close to that of the
      privileged group (Caucasians). This indicates relatively fair treatment in
      the algorithm's overall positive predictions.

4. **Accuracy parity**: Compares the accuracy of each unprivileged group's
  predictions with the privileged group.

    The formula for the accuracy is $A = \frac{TP + TN}{TP + FP + TN + FN}$,
      and the accuracy parity for unprivileged group $i$ is given by
      $AP = \frac{A_{i}}{A_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "ap"
)
    ```

    ***Interpretation:***
    - African American: The accuracy parity for African Americans is 1.021,
      suggesting their accuracy is very similar to the privileged group
      (Caucasians). This indicates fair treatment concerning overall accuracy.
    - *Asian*: The accuracy parity for Asians is 1.1266, suggesting their
      accuracy is slightly higher than for Caucasians, indicating potential
      favoritism in overall accuracy.
    - *Hispanic*: The accuraty parity for Hispanics is 1.0351, suggesting their
      accuracy is slightly higher than for Caucasians, indicating potential
      favoritism in overall accuracy.

5. **False negative rate parity (Treatment equality)**: Compares the false
  negative rates of each unprivileged group with the privileged group.

    The formula for the false negative rate is $FNR = \frac{FN}{TP + FN}$, and
      the false negative rate parity for unprivileged group $i$ is given by
      $FNRP = \frac{FNR_{i}}{FNR_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "fnrp"
)
    ```

    ***Interpretation:***
    - *African American*: The parity of false negative rates (FNRs) for African
      Americans compared to Caucasians is 0.46866. A value lower than 1 suggests
      that African Americans are less likely to be falsely classified as
      non-reoffenders, indicating potential bias against this group in this
      aspect.
    - *Asian*: The parity for Asians is 1.4205, indicating that they are more
      likely to be falsely classified as non-reoffenders compared to Caucasians,
      suggesting potential underestimation of reoffenders among Asians.
    - *Hispanic*: The FNR parity for Hispanics is 1.0121, indicating
      relatively similar rates as the privileged group (Caucasians), suggesting
      fair treatment in this aspect.

6. **False positive rate parity**: Compares the false positive rates (e.g., for
  non-reoffenders) of each unprivileged group with the privileged group.

    The formula for the false positive rate is $FPR = \frac{FP}{TN + FP}$, and
      the false positive rate parity for unprivileged group $i$ is given by
      $FPRP = \frac{FPR_{i}}{FPR_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "fprp"
)
    ```

    ***Interpretation:***
    - *African American*: The false positive rate parity for African
      Americans is 1.8739. This indicates that African Americans are
      approximately 1.87 times more likely to be falsely predicted as
      reoffenders than Caucasians. This suggests potential bias in the
      algorithm's false positive predictions in favor of African Americans.
    - *Asian*: The parity for Asians is 0.39222, indicating that they are less
      likely to be falsely predicted as reoffenders compared to Caucasians. This
      suggests potential fair treatment of Asians in false positive predictions.
    - *Hispanic*: The false positive rate parity for Hispanics is 0.85983,
      suggesting they are less likely to be falsely predicted as reoffenders
      compared to Caucasians. This indicates potential fair treatment of
      Hispanics in false positive predictions.

7. **True positive rate parity (Equal opportunity)**: Compares the true positive
  rates (e.g., for reoffenders) of each unprivileged group with the privileged
  group.

    The formula for the true positive rate is $TPR = \frac{TP}{TP + FN}$, and
     the true positive rate parity for unprivileged group $i$ is given by
     $TPRP = \frac{TPR_{i}}{TPR_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "tprp"
)
    ```

    ***Interpretation:***
    - *African American*: The true positive rate parity for African
      Americans is 1.5943. This indicates that African Americans are
      approximately 1.59 times more likely to be correctly predicted as
      reoffenders than Caucasians. This suggests potential favoritism towards
      African Americans in true positive predictions made by the algorithm.
    - *Asian*: The parity for Asians is 0.52964, indicating that they are less
      likely to be correctly predicted as reoffenders compared to Caucasians.
      This suggests potential underestimation of reoffenders among Asians by the
      algorithm.
    - *Hispanic:* The true positive rate parity for Hispanics is 0.98642,
      suggesting their true positive rate is very close to that of the
      privileged group (Caucasians). This indicates relatively fair treatment
      in the algorithm's true positive predictions.

8. **Negative predicted value parity**: Compares the negative predicted value
  (e.g., for non-reoffenders) of each unprivileged group with that of the
  privileged group.

    The formula for the negative predicted value is $NPV = \frac{TN}{TN + FN}$,
      and the negative predicted value parity for unprivileged group $i$ is
      given by $NPVP = \frac{NPV_{i}}{NPV_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "npvp"
)
    ```

    ***Interpretation:***
    - *African American*: The negative predicted value parity for African
      Americans is 0.98013. A value close to 1 indicates that the negative
      predicted value for African Americans is very similar to the privileged
      group (Caucasians). This suggests fair treatment in predicting
      non-reoffenders among African Americans.
    - *Asian*: The negative predicted value parity for Asians is 1.1163,
      indicating that their negative predicted value is slightly higher than for
      Caucasians. This could suggest potential favoritism towards Asians in
      predicting non-reoffenders.
    - *Hispanic*: The negative predicted value parity for Hispanics is 1.0326,
      suggesting that their negative predicted value is slightly higher than for
      Caucasians. This indicates potential favoritism towards Hispanics in
      predicting non-reoffenders.

9. **Specificity parity (True negative rate parity)**: Compares the specificity
  (true negative rate) of each unprivileged group with the privileged group.

    The formula for the specificity is $S = \frac{TN}{TN + FP}$, and the
      specificity parity for unprivileged group $i$ is given by
      $SP = \frac{S_{i}}{S_{privileged}}$.

    ```{r}
model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "sp"
)
    ```

    ***Interpretation:***
    - *African American*: The specificity parity for African Americans is
      0.75105. A value lower than 1 indicates that the specificity for African
      Americans is lower than for Caucasians. This suggests potential bias in
      correctly identifying non-reoffenders among African Americans.
    - *Asian*: The specificity parity for Asians is 1.1731, indicating
      their specificity is slightly higher than for Caucasians. This could
      suggest potential favoritism in correctly identifying non-reoffenders
      among Asians.
    - *Hispanic*: The specificity parity for Hispanics is 1.0399,
      suggesting that their specificity is very close to the privileged group.
      This indicates relatively fair treatment in correctly identifying
      non-reoffenders among Hispanics.

### Bayesian Analysis

Bayesian inference, which is supported for all metrics except demographic
parity, provides credible intervals and Bayes factors for the fairness metrics
and tests [@jamil_2017]. Similar to other functions in **jfa**, a Bayesian
analysis can be conducted using a default prior by setting `prior = TRUE`. The
prior distribution in this analysis is specified on the log odds ratio and can
be modified by setting `prior = 1` (equal to `prior = TRUE`), or providing a
number greater than one that represents the prior concentration parameter (for
example, `prior = 3`). The larger the concentration parameter, the more the
prior distribution is focused around zero, implying that it assigns a higher
probability to the scenario of equal fairness metrics.

```{r}
x <- model_fairness(
  data = compas,
  protected = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  privileged = "Caucasian",
  positive = "yes",
  metric = "pp",
  prior = TRUE
)
print(x)
```

The Bayes factor $BF_{01}$, used in Bayesian inference, quantifies the evidence
supporting algorithmic fairness (i.e., equal fairness metrics across all groups)
over algorithmic bias. Conversely, $BF_{10}$ quantifies the evidence supporting
algorithmic bias over algorithmic fairness. By default, **jfa** reports
$BF_{10}$, but $BF_{01}$ = $\frac{1}{BF_{10}}$. The output above shows the
resulting Bayes factor ($BF_{01}$) in favor of rejecting the null hypothesis of
algorithmic fairness. As shown, $BF_{01}$ > 1000, indicating extreme evidence
against the hypothesis of equal fairness metrics between the groups. 

The prior and posterior distribution for the group comparisons can be
visualized by invoking `plot(..., type = "posterior")`.

```{r, fig.align="center", fig.height=4, fig.width=6}
plot(x, type = "posterior")
```

Additionally, the robustness of the Bayes factor to the choice of prior
distribution can be examined by calling `plot(..., type = "robustness")`, as
shown in the code below. Finally, the auditor has the option to conduct a
sequential analysis using `plot(..., type = "sequential")`.

```{r, fig.align="center", fig.height=4, fig.width=6}
plot(x, type = "robustness")
```

## Function: `fairness_selection()`

The `fairness_selection()` function offers a method to select a fairness measure 
tailored to a specific context and dataset by answering the questions in the 
developed decision-making workflow. The fairness measure that can be selected 
include disparate impact, equalized odds, false positive rate parity, false 
negative rate parity, predictive rate parity, equal opportunity, specificity 
parity, negative predictive rate parity, accuracy parity [@castelnovo_2022;
@feldman_2015; @friedler_2019; @hardt_2016, @verma_2018]. After answering the 
questions in the decision-making workflow and selecting the fairness measure to 
apply, a graphical representation of the followed path can be created based 
on the responses.

As mentioned earlier, not all fairness measures are equally suitable for every 
audit situation. For this reason, the `fairness_selection()` function we propose 
can assist the auditor in selecting the most appropriate measure for the 
specific audit at hand. To demonstrate the usage of the 
`fairness_selection()` function, we will answer each necessary question in the
decision-making workflow, explaining the reasoning behind each response.

The first question in the workflow checks if the dataset includes the ground 
truth information (i.e., the true classification values). The data available in 
this dataset were collected during a retrospective study, so we have information 
on whether offenders committed a crime within 2 years of being released 
(`TwoYrRecidivism`). Therefore, we can answer the first question with `Yes` (in 
the function the value `1` is used to indicate `Yes`).

The second question focuses on the type of classification of interest: correct,
incorrect, or both. We can imagine a scenario where the analysis of this dataset
is tied to U.S. Attorney General Eric Holder's concerns about potential 
discrimination against certain social groups resulting from anomalies 
in AI-generated classifications, particularly errors in classifying offenders. 
Given the importance of identifying such irregularities, our focus here is on 
the incorrect classification of offenders.  Therefore, we can answer the first 
question with `Incorrect Classification` (in the function the value `2` is 
used to indicate `Incorrect Classification`).

Given the answer to the second question and following the decision-making 
workflow path, we can skip the third question and proceed directly to the 
fourth and final question.

The fourth question addresses the costs of classification errors meaning which 
classification error results in highest cost. The classification errors can be 
made when classifying someone who would not commit another crime within two 
years of release as someone who would (leading a longer incarceration or 
failing to release someone from prison in a timely manner) and when classifying 
a dangerous individual who would commit another crime as someone who would not 
(leading to an early release or shorter incarceration period). To answer the 
question, we assume a scenario where AI is being developed and promoted within 
the U.S. criminal justice system, with the goal of reducing incarceration and 
alleviating prison overcrowding. Thus, higher costs are associated with
mistakenly incarcerating individuals who would not reoffend within two years 
of release, as this results in unnecessary prolonged imprisonment. Therefore,
we can answer the fourth question with `False Positive` (in the function the 
value `1` is used to indicate `False Positive`).

```{r}
measure <- fairness_selection(q1 = 1, q2 = 2, q4 = 1)
print(measure)
```

It is worth noting that the function can also be called without any 
arguments (`measure <- fairness_selection()`). In this case, an interactive mode 
is activated, allowing the user to answer the necessary questions in real time. 
By applying this function, an object of class `jfaFairnessSelection` is created.

Additionally, the `print` function, applied to an object of class 
`jfaFairnessSelection`, allows us to display not only which fairness measure is 
selected as the most suitable for the audit situation, but also a summary of 
how we answered the questions in the decision-making workflow.

Finally, as mentioned earlier, it is possible to visualize the path followed 
by answering the questions in the decision-making workflow. This is done by 
applying the `plot` function an object of class `jfaFairnessSelection`.

```{r, fig.align="center"}
plot(measure)
```

## References
<div id="refs"></div>
