---
title: "Evaluating samples with partial misstatements"
author: Koen Derks
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Evaluating samples with partial misstatements}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{jfa}
  %\VignetteKeywords{audit, evaluation, jfa}
  %\VignettePackage{jfa}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(jfa)
```

## Introduction

This vignette illustrates the efficient evaluation of samples with partial
misstatements using the `evaluation()` function from the **jfa** package.

<p align='center'><img src='evaluation.png' alt='evaluation' width='100%'></p>

In auditing, the objective of evaluation is typically to estimate the
misstatement in the population based on a sample or to test the misstatement
against a critical upper limit, known as performance materiality.

## Modeling partial misstatements

Partial misstatements occur when there is only a partial discrepancy between the
true value of an item in the sample and its recorded value. From a data
perspective, this implies that misstatements are often non-binary. However,
there is usually a large number of correct items in the population.

As an example, we examine a realistic sample from a financial audit using the
`allowances` data set included in the package. For clarity, we perform some data
pre-processing to arrive at an example population.

```{r}
data("allowances")
population <- allowances
population <- population[population[["bookValue"]] > 0, ]
population <- population[!is.na(population[["auditValue"]]), ]
population <- population[population[["auditValue"]] > 0, c(1, 3, 4)]
head(population)
```

To decide whether the misstatement in the population is lower than the
performance materiality of ten percent, a sample of 50 items is selected from the
population, each consisting of a book value and a true value. In this case, the
sample is selected using a fixed interval sampling method in which the items in
the population are randomized before selection.

```{r}
set.seed(1)
sample <- selection(
  data = population, size = 50,
  method = "interval", units = "values", values = "bookValue",
  randomize = TRUE
)$sample
```

As can often be seen in practice, the sample contains many taints (i.e.,
proportional misstatements) that are zero, and only some taints that are
non-zero. The table below shows the distribution of the taints.

```{r}
sample[["difference"]] <- sample[["bookValue"]] - sample[["auditValue"]]
sample[["taint"]] <- sample[["difference"]] / sample[["bookValue"]]
table(round(sample[["taint"]], 3))
```

Taking into account the abundance of zeros in the data when modeling
misstatement in the population can increase efficiency. In the sections below,
we demonstrate several methods that can be used to achieve this.

### Beta distribution

The simplest way to analyze partial misstatements is to aggregate them and use
the sum of partial misstatements (i.e., the total taint) to project to the
population. In the Bayesian framework, this involves using a prior and updating
it with the aggregated information from the sample. For the binomial likelihood,
this is done using `method = "binomial"` in combination with `prior = TRUE`.

```{r}
eval_beta <- evaluation(
  method = "binomial", data = sample, materiality = 0.1,
  values = "bookValue", values.audit = "auditValue",
  prior = TRUE
)
print(eval_beta)
```

As shown in the output, the most likely misstatement is estimated to be 8.46
percent, with a 95 percent upper bound of 17.63 percent. Note that this estimate
is not very efficient, as it does not take into account the information in the
distribution of the taints. The prior and posterior distribution can be
visualized via the `plot(..., type = "posterior")` function.

```{r fig.align="center", fig.height=4, fig.width=6}
plot(eval_beta, type = "posterior")
```

### Stringer bound

The Stringer bound is a method to improve efficiency by taking into account the
differences between the taints. While conservative, this method can reduce the
upper bound on the misstatement compared to the beta distribution. This can be
done using `method = "stringer"` in combination with `prior = FALSE`.

```{r}
eval_stringer <- evaluation(
  method = "stringer", data = sample, materiality = 0.1,
  values = "bookValue", values.audit = "auditValue"
)
print(eval_stringer)
```

As shown in the output, the most likely misstatement is estimated to be 8.46
percent, with a 95 percent upper bound of 17.23 percent. Since the upper bound of
the Stringer method is (slightly) lower than that of the beta distribution, the
Stringer bound is more efficient.

### Beta hurdle model

Efficiency can be further improved by explicitly modeling the probability of a
taint being zero (i.e., zero-inflation). This is a more realistic method
because, in practice, most taints are zero. A zero-inflated variant of the beta
distribution can be fitted using `method = "hurdle.beta"`, which requires that
`prior = TRUE` (or an object created via the `auditPrior()` function). Note that
this method also requires specifying the number of items (`N.items`) and the
number of monetary units (`N.units`) in the population.

In the context of a “zero-inflated Poisson” model (i.e., `inflated.poisson` in
`evaluation()`), we essentially incorporate "extra" zeros into the zeros already
present in the Poisson distribution. However, this concept doesn't directly
apply to a "zero-inflated beta" model. Since beta regression cannot accommodate
zeros, there are no zeros to inflate. A more appropriate term for this model
is a Hurdle model. This terminology better reflects the fact that the model has
to overcome the hurdle of zero values which are not inherently part of the beta
distribution.

```{r}
eval_zib <- evaluation(
  method = "hurdle.beta", data = sample, materiality = 0.1,
  values = "bookValue", values.audit = "auditValue",
  N.items = nrow(population), N.units = sum(population$bookValue),
  prior = TRUE
)
print(eval_zib)
```

As shown in the output, the most likely misstatement is estimated to be around
6.9 percent, with a 95 percent upper bound of around 14.44 percent. Since the
upper bound of the hurdle beta distribution is lower than that of the Stringer
bound, the hurdle beta distribution is more efficient. The prior and posterior
distribution can be visualized via the `plot()` function.

```{r fig.align="center", fig.height=4, fig.width=6}
plot(eval_zib, type = "posterior")
```
