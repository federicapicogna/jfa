% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_bias.R
\name{model_bias}
\alias{model_bias}
\title{Algorithm Auditing: Fairness Metrics and Bias Detection}
\usage{
model_bias(
  data,
  grouping,
  observed,
  predicted,
  reference,
  tolerance = 0.8
)
}
\arguments{
\item{data}{The input data.}

\item{grouping}{The column name indicating the grouping variable.}

\item{observed}{The column name indicating the observed class labels.}

\item{predicted}{The column name indicating the predicted class labels.}

\item{reference}{The reference class for computing fairness metrics.}

\item{tolerance}{The tolerance value for determining fairness.}
}
\value{
An object of class \code{jfaModelBias} containing:

\item{reference}{The reference group for computing fairness metrics.}
\item{confusion.matrix}{A list of confusion matrices for each group.}
\item{performance}{A data frame containing performance metrics for each
  group, including accuracy, precision, recall, and F1 score.}
\item{fairness}{A data frame containing fairness metrics for each group,
  including demographic parity, predictive parity, predictive rate parity,
  accuracy parity, false negative rate parity, false positive rate parity,
  true positive rate parity, negative predicted value parity, and statistical
  parity.}
\item{ratio}{A data frame containing fairness ratios for each metric,
  comparing each group to the reference group.}
\item{tolerance}{The tolerance value used to determine fairness.}
\item{data.name}{The name of the input data object.}
}
\description{
This function detects bias in algorithmic decision-making
systems by computing various fairness metrics. It provides an assessment of
fairness across different groups based on the observed and predicted
outcomes. The function allows for evaluating fairness using metrics such as
demographic parity, predictive parity, predictive rate parity, accuracy
parity, false negative rate parity, false positive rate parity, true positive
rate parity, negative predicted value parity, and statistical parity and
decide whether groups are fair to a certain degree and within a certain
tolerance.
}
\details{
The following fairness metrics are computed:

\itemize{
  \item{Demographic parity}{measures whether the observed variable is
    distributed equally across different groups.}
  \item{Predictive parity}{measures whether the predicted variable is
    distributed equally across different groups.}
  \item{Predictive rate parity}{measures whether the positive prediction rate
    is the same across different groups.}
  \item{Accuracy parity}{measures whether the overall accuracy is the same
    across different groups.}
  \item{False negative rate parity}{measures whether the false negative rate
    is the same across different groups.}
  \item{False positive rate parity}{measures whether the false positive rate
    is the same across different groups.}
  \item{True positive rate parity}{measures whether the true positive rate is
    the same across different groups.}
  \item{Negative predicted value parity}{measures whether the negative
    predicted value is the same across different groups.}
  \item{Statistical parity}{measures whether the predicted variable is
    distributed equally across different groups.}
}
}
\examples{
library(randomForest)
data(iris)
set.seed(1)

iris$Priveledged <- factor(sample(c("Yes", "No", "Unknown"), nrow(iris), TRUE))
train_index <- sample.int(nrow(iris), 100)
train <- iris[train_index, ]
test <- iris[-train_index, ]
model <- randomForest(Species ~ ., data = train)
iris$Predicted <- factor(predict(model, newdata = test))

model_bias(iris, "Priveledged", "Species", "Predicted",
  reference = "No"
)
}
\author{
Koen Derks, \email{k.derks@nyenrode.nl}
}
\keyword{algorithm}
\keyword{audit}
\keyword{bias}
\keyword{fairness}
\keyword{model}
\keyword{performance}
