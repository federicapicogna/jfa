% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_fairness.R
\name{model_fairness}
\alias{model_fairness}
\title{Algorithm Auditing: Fairness Metrics and Bias Detection}
\usage{
model_fairness(
  data,
  sensitive,
  target,
  predictions,
  reference = NULL,
  positive = NULL,
  materiality = 0.8,
  alternative = c("two.sided", "greater", "less"),
  conf.level = 0.95
)
}
\arguments{
\item{data}{a data frame containing the input data.}

\item{sensitive}{a character specifying the column name in \code{data}
indicating the sensitive variable.}

\item{target}{A character specifying the column name in \code{data}
indicating the actual values of the target (to be predicted) variable.}

\item{predictions}{a character specifying the column name in \code{data}
indicating the predicted values of the target variable.}

\item{reference}{a character specifying the class in the column
\code{sensitive} used as the reference class for computing the fairness
metrics. If \code{NULL} (the default), the first factor level of the
\code{sensitive} column is used as the reference group.}

\item{positive}{a character specifying the positive class in the column
\code{target} used for computing the fairness metrics. If \code{NULL} (the
default), the first factor level of the \code{target} column is used as the
positive class.}

\item{materiality}{a numeric value between 0 and 1 specifying the
materiality used to decide whether the statistics are out of bound. The
tolerance range is defined on the parity statistics as
\code{materiality} and \code{1 / materiality}.}

\item{alternative}{the type of confidence interval to produce. Possible
options are \code{two.sided} (the default), \code{greater} and \code{less}.}

\item{conf.level}{a numeric value between 0 and 1 specifying the
confidence level (i.e., 1 - audit risk / detection risk).}
}
\value{
An object of class \code{jfaModelBias} containing:

\item{reference}{The reference group for computing the fairness metrics.}
\item{positive}{The positive class used in computing the fairness metrics.}
\item{alternative}{The type of confidence interval.}
\item{confusion.matrix}{A list of confusion matrices for each group.}
\item{performance}{A data frame containing performance metrics for each
  group, including accuracy, precision, recall, and F1 score.}
\item{metrics}{A data frame containing fairness metrics for each group,
  including demographic parity, proportional parity, predictive rate parity,
  accuracy parity, false negative rate parity, false positive rate parity,
  true positive rate parity, negative predicted value parity, and statistical
  parity.}
\item{parity}{A data frame containing fairness parity for each metric,
  comparing each group to the reference group.}
\item{odds.ratio}{A data frame containing odds ratio of the fairness parity
  for each metric, comparing each group to the reference group.}
\item{materiality}{The materiality value used to determine the out of bounds
  metrics.}
\item{data.name}{The name of the input data object.}
}
\description{
This function detects bias in algorithmic decision-making
systems by computing various model-agnostic fairness metrics. These measures
quantify fairness across different groups based on the observed and predicted
outcomes of the model. The calculated metrics are commonly used and include
demographic parity, proportional parity, predictive rate parity, accuracy
parity, false negative rate parity, false positive rate parity, true positive
rate parity, negative predicted value parity, and specificity parity. In
addition, the user can specify a materiality threshold to decide whether
the algorithm is fair to a certain degree and within a certain range.
Currently, this function only supports binary classification.
}
\details{
The following model-agnostic fairness metrics are computed based on
  the confusion matrix for each sensitive group, using the  true positives
  (TP), false positives (FP), true negative (TN) and false negatives (FN).

  \itemize{
    \item{Demographic parity: }{measures whether the observed variable is
    distributed equally across different groups, calculated as TP + FP.}
    \item{Proportional parity: }{measures whether the positive rate is
      distributed equally across different groups, calculated as (TP + FP) /
      (TP + FP + TN + FN).}
    \item{Predictive rate parity: }{measures whether the positive prediction
      rate is the same across different groups, calculated as TP / (TP + FP).}
    \item{Accuracy parity: }{measures whether the overall accuracy is the same
      across different groups, calculated as (TP + TN) / (TP + FP + TN + FN).}
    \item{False negative rate parity: }{measures whether the false negative
      rate is the same across different groups, calculated as FN / (FP + FN).}
    \item{False positive rate parity: }{measures whether the false positive
      rate is the same across different groups, calculated as FP / (TN + FP).}
    \item{True positive rate parity: }{measures whether the true positive rate
      is the same across different groups, calculated as TP / (TP + FN).}
    \item{Negative predicted value parity: }{measures whether the negative
      predicted value is the same across different groups, calculated as TN /
      (TN + FN).}
    \item{Specificity parity: }{measures whether the true positive rate
      is the same across different groups, calculated as TN / (TN + FP).}
  }

  Note that, in an audit context, not all fairness measures are equally
  appropriate in all situations. The fairness tree below aids in choosing
  which fairness measure is appropriate for the situation at hand (B端y端k,
  2023).

  \if{html}{\figure{fairness-tree.png}{options: width="100\%" alt="fairness-tree"}}
  \if{latex}{\figure{fairness-tree.pdf}{options: width=5in}}
}
\examples{
model_fairness(compas, "Ethnicity", "TwoYrRecidivism", "Predicted",
  reference = "Caucasian", positive = "yes"
)
}
\references{
B端y端k, S. (2023). \emph{Automatic Fairness Criteria and Fair
  Model Selection for Critical ML Tasks}, Master Thesis, Utrecht University.

Pessach, D. & Shmueli, E. (2022). A review on fairness in machine
  learning. \emph{ACM Computing Surveys}, 55(3), 1-44. \doi{10.1145/3494672}
}
\author{
Koen Derks, \email{k.derks@nyenrode.nl}
}
\keyword{algorithm}
\keyword{audit}
\keyword{bias}
\keyword{fairness}
\keyword{model}
\keyword{performance}
