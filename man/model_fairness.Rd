% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_fairness.R
\name{model_fairness}
\alias{model_fairness}
\title{Algorithm Auditing: Fairness Metrics and Bias Detection}
\usage{
model_fairness(
  data,
  sensitive,
  target,
  predictions,
  reference = NULL,
  positive = NULL,
  metric = c(
    "prp", "pp", "ap", "fnrp", "fprp",
    "tprp", "npvp", "sp", "dp"
  ),
  alternative = c("two.sided", "greater", "less"),
  conf.level = 0.95,
  prior = FALSE
)
}
\arguments{
\item{data}{a data frame containing the input data.}

\item{sensitive}{a character specifying the column name in \code{data}
indicating the sensitive variable.}

\item{target}{A character specifying the column name in \code{data}
indicating the actual values of the target (to be predicted) variable.}

\item{predictions}{a character specifying the column name in \code{data}
indicating the predicted values of the target variable.}

\item{reference}{a character specifying the class in the column
\code{sensitive} used as the reference class for computing the fairness
metrics. If \code{NULL} (the default), the first factor level of the
\code{sensitive} column is used as the reference group.}

\item{positive}{a character specifying the positive class in the column
\code{target} used for computing the fairness metrics. If \code{NULL} (the
default), the first factor level of the \code{target} column is used as the
positive class.}

\item{metric}{a character (vector) indicating the fairness metrics(s)
to compute. See the Details section for more information.}

\item{alternative}{the type of confidence interval to produce. Possible
options are \code{two.sided} (the default), \code{greater} and \code{less}.}

\item{conf.level}{a numeric value between 0 and 1 specifying the
confidence level (i.e., 1 - audit risk / detection risk).}

\item{prior}{a logical specifying whether to use a prior distribution.
If this argument is specified as \code{FALSE} (default), the function
performs classical evaluation. If this argument is \code{TRUE}, this
function performs Bayesian evaluation using a default prior.}
}
\value{
An object of class \code{jfaModelFairness} containing:

\item{reference}{The reference group for computing the fairness metrics.}
\item{positive}{The positive class used in computing the fairness metrics.}
\item{alternative}{The type of confidence interval.}
\item{confusion.matrix}{A list of confusion matrices for each group.}
\item{performance}{A data frame containing performance metrics for each
  group, including accuracy, precision, recall, and F1 score.}
\item{metric}{A data frame containing, for each group, the estimates of the
  fairness metric along with the associated confidence / credible interval.}
\item{parity}{A data frame containing, for each sensitive group, the parity
  ratio and associated confidence / credible interval when compared to the
  reference group.}
\item{odds.ratio}{A data frame containing, for each sensitive group, the odds
  ratio of the fairness meatrics and associated confidence/credible interval,
  along with any inferential measures, for the comparison to the reference
  group.}
\item{data.name}{The name of the input data object.}
}
\description{
This function aims to assess fairness and bias in algorithmic
decision-making systems by computing and testing one of several
model-agnostic fairness metrics. These metrics quantify fairness across
groups in the data based on the predictions of the algorithm in several ways.
The calculated metrics are commonly used and include predictive rate parity,
proportional parity, true positive rate parity, accuracy parity, false
negative rate parity, false positive rate parity, true positive
rate parity, negative predicted value parity, specificity parity, and
demographic parity. Currently, this function only supports binary
classification.
}
\details{
The following model-agnostic fairness metrics are computed based on
  the confusion matrix for each sensitive group, using the  true positives
  (TP), false positives (FP), true negative (TN) and false negatives (FN).

  \itemize{
    \item{Predictive rate parity (\code{prp}): }{measures whether the positive prediction
      rate is the same across different groups, calculated as TP / (TP + FP).}
    \item{Proportional parity (\code{pp}): }{measures whether the positive rate is
      distributed equally across different groups, calculated as (TP + FP) /
      (TP + FP + TN + FN).}
    \item{Accuracy parity (\code{ap}): }{measures whether the overall accuracy is the same
      across different groups, calculated as (TP + TN) / (TP + FP + TN + FN).}
    \item{False negative rate parity (\code{fnrp}): }{measures whether the false negative
      rate is the same across different groups, calculated as FN / (FP + FN).}
    \item{False positive rate parity (\code{fprp}): }{measures whether the false positive
      rate is the same across different groups, calculated as FP / (TN + FP).}
    \item{True positive rate parity (\code{tprp}): }{measures whether the true positive rate
      is the same across different groups, calculated as TP / (TP + FN).}
    \item{Negative predicted value parity (\code{npvp}): }{measures whether the negative
      predicted value is the same across different groups, calculated as TN /
      (TN + FN).}
    \item{Specificity parity (\code{sp}): }{measures whether the true positive rate
      is the same across different groups, calculated as TN / (TN + FP).}
    \item{Demographic parity (\code{dp}): }{measures whether the observed variable is
      distributed equally across different groups, calculated as TP + FP.}
  }

  Note that, in an audit context, not all fairness measures are equally
  appropriate in all situations. The fairness tree below aids in choosing
  which fairness measure is appropriate for the situation at hand (B端y端k,
  2023).

  \if{html}{\figure{fairness-tree.png}{options: width="100\%" alt="fairness-tree"}}
  \if{latex}{\figure{fairness-tree.pdf}{options: width=5in}}
}
\examples{
# Frequentist estimation of specificy parity
model_fairness(compas, "Gender", "TwoYrRecidivism", "Predicted",
  reference = "Male", positive = "yes", metric = "sp"
)

# Bayesian estimation of predictive rate parity
model_fairness(compas, "Ethnicity", "TwoYrRecidivism", "Predicted",
  reference = "Caucasian", positive = "yes", metric = "prp", prior = TRUE
)
}
\references{
B端y端k, S. (2023). \emph{Automatic Fairness Criteria and Fair
  Model Selection for Critical ML Tasks}, Master Thesis, Utrecht University.

Pessach, D. & Shmueli, E. (2022). A review on fairness in machine
  learning. \emph{ACM Computing Surveys}, 55(3), 1-44. \doi{10.1145/3494672}
}
\author{
Koen Derks, \email{k.derks@nyenrode.nl}
}
\keyword{algorithm}
\keyword{audit}
\keyword{bias}
\keyword{fairness}
\keyword{model}
\keyword{performance}
