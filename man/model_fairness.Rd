% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_fairness.R
\name{model_fairness}
\alias{model_fairness}
\title{Algorithm Auditing: Fairness Metrics and Bias Detection}
\usage{
model_fairness(
  data,
  sensitive,
  target,
  predictions,
  reference = NULL,
  positive = NULL,
  metric = c(
    "prp", "pp", "ap", "fnrp", "fprp",
    "tprp", "npvp", "sp", "dp"
  ),
  alternative = c("two.sided", "greater", "less"),
  conf.level = 0.95,
  prior = FALSE
)
}
\arguments{
\item{data}{a data frame containing the input data.}

\item{sensitive}{a character specifying the column name in \code{data}
indicating the sensitive variable.}

\item{target}{A character specifying the column name in \code{data}
indicating the actual values of the target (to be predicted) variable.}

\item{predictions}{a character specifying the column name in \code{data}
indicating the predicted values of the target variable.}

\item{reference}{a character specifying the class in the column
\code{sensitive} used as the reference class for computing the fairness
metrics. If \code{NULL} (the default), the first factor level of the
\code{sensitive} column is used as the reference group.}

\item{positive}{a character specifying the positive class in the column
\code{target} used for computing the fairness metrics. If \code{NULL} (the
default), the first factor level of the \code{target} column is used as the
positive class.}

\item{metric}{a character (vector) indicating the fairness metrics(s)
to compute. See the Details section for more information.}

\item{alternative}{the type of confidence interval to produce. Possible
options are \code{two.sided} (the default), \code{greater} and \code{less}.}

\item{conf.level}{a numeric value between 0 and 1 specifying the
confidence level (i.e., 1 - audit risk / detection risk).}

\item{prior}{a logical specifying whether to use a prior distribution,
or a numeric value equal to or larger than 1 specifying the prior
concentration parameter. If this argument is specified as \code{FALSE}
(default), classical estimation is performed and if it is \code{TRUE},
Bayesian estimation using a default prior with concentration parameter 1 is
performed.}
}
\value{
An object of class \code{jfaModelFairness} containing:

\item{reference}{The reference group for computing the fairness metrics.}
\item{positive}{The positive class used in computing the fairness metrics.}
\item{alternative}{The type of confidence interval.}
\item{confusion.matrix}{A list of confusion matrices for each group.}
\item{performance}{A data frame containing performance metrics for each
  group, including accuracy, precision, recall, and F1 score.}
\item{metric}{A data frame containing, for each group, the estimates of the
  fairness metric along with the associated confidence / credible interval.}
\item{parity}{A data frame containing, for each sensitive group, the parity
  ratio and associated confidence / credible interval when compared to the
  reference group.}
\item{odds.ratio}{A data frame containing, for each sensitive group, the odds
  ratio of the fairness meatrics and associated confidence/credible interval,
  along with any inferential measures, for the comparison to the reference
  group.}
\item{measure}{The abbreviation of the selected fairness metric.}
\item{data.name}{The name of the input data object.}
}
\description{
This function aims to assess fairness and bias in algorithmic
decision-making systems by computing and testing the equality in one of
several model-agnostic fairness metrics. These metrics aim to quantify
fairness across sensitive groups in the data based on the predictions of the
algorithm. The metrics that can be calculated include predictive rate parity,
proportional parity, accuracy parity, false negative rate parity, false
positive rate parity, true positive rate parity, negative predicted value
parity, specificity parity, and demographic parity. Currently, this function
only supports binary classification. The function returns an object of class
\code{jfaModelBias} that can be used with associated \code{summary()} and
\code{plot()} methods.
}
\details{
The following model-agnostic fairness metrics are computed based on
  the confusion matrix for each sensitive group, using the true positives
  (TP), false positives (FP), true negative (TN) and false negatives (FN).
  See Pessach & Shmueli (2022) for a more detailed explanation of the
  individual metrics. The equality of metrics across groups is done according
  to the methodology described in Fisher (1970) and Jamil et al. (2017).

  \itemize{
    \item{Predictive rate parity (\code{prp}): }{calculated as TP / (TP +
      FP), quantifies whether the positive prediction rate is the same across
      groups.}
    \item{Proportional parity (\code{pp}): }{calculated as (TP + FP) / (TP +
      FP + TN + FN), quantifies whether the positive prediction rate is equal
      across groups.}
    \item{Accuracy parity (\code{ap}): }{calculated as (TP + TN) / (TP + FP +
      TN + FN), quantifies whether the accuracy is the same across groups.}
    \item{False negative rate parity (\code{fnrp}): }{calculated as FN / (FP
      + FN), quantifies whether the false negative rate is the same across
      groups.}
    \item{False positive rate parity (\code{fprp}): }{calculated as FP / (TN
      + FP), quantifes whether the false positive rate is the same across
      groups.}
    \item{True positive rate parity (\code{tprp}): }{calculated as TP / (TP +
      FN), quantifies whether the true positive rate is the same across
      groups.}
    \item{Negative predicted value parity (\code{npvp}): }{calculated as TN /
      (TN + FN), quantifies whether the negative predicted value is equal
      across groups.}
    \item{Specificity parity (\code{sp}): }{calculated as TN / (TN + FP),
      quantifies whether the true positive rate is the same across groups.}
    \item{Demographic parity (\code{dp}): }{calculated as TP + FP, quantifies
      whether the positive predictions are equal across groups.}
  }

  Note that, in an audit context, not all fairness measures are equally
  appropriate in all situations. The fairness tree below aids in choosing
  which fairness measure is appropriate for the situation at hand (B端y端k,
  2023).

  \if{html}{\figure{fairness-tree.png}{options: width="100\%" alt="fairness-tree"}}
  \if{latex}{\figure{fairness-tree.pdf}{options: width=5in}}
}
\examples{
# Frequentist test of specificy parity
model_fairness(
  data = compas,
  sensitive = "Gender",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  reference = "Male",
  positive = "yes",
  metric = "sp"
)

# Bayesian test of predictive rate parity
model_fairness(
  data = compas,
  sensitive = "Ethnicity",
  target = "TwoYrRecidivism",
  predictions = "Predicted",
  reference = "Caucasian",
  positive = "yes",
  metric = "prp",
  prior = TRUE
)
}
\references{
B端y端k, S. (2023). \emph{Automatic Fairness Criteria and Fair
  Model Selection for Critical ML Tasks}, Master Thesis, Utrecht University.

Calders, T., & Verwer, S. (2010). Three naive Bayes approaches
  for discrimination-free classification. In \emph{Data Mining and Knowledge
  Discovery}. Springer Science and Business Media LLC.
  \doi{10.1007/s10618-010-0190-x}

Chouldechova, A. (2017). Fair Pprediction with disparate impact:
  A study of bias in recidivism prediction instruments. In \emph{Big Data}.
  Mary Ann Liebert Inc. \doi{10.1089/big.2016.0047}

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &
  Venkatasubramanian, S. (2015). Certifying and removing disparate impact. In
  \emph{Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining}. \doi{10.1145/2783258.2783311}

Friedler, S. A., Scheidegger, C., Venkatasubramanian, S.,
  Choudhary, S., Hamilton, E. P., & Roth, D. (2019). A comparative study of
  fairness-enhancing interventions in machine learning. In \emph{Proceedings
  of the Conference on Fairness, Accountability, and Transparency}.
  \doi{10.1145/3287560.3287589}

Fisher, R. A. (1970). \emph{Statistical Methods for Research
  Workers}. Oliver & Boyd.

Jamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., &
  Wagenmakers, E. J. (2017). Default "Gunel and Dickey" Bayes factors for
  contingency tables. \emph{Behavior Research Methods}, 49, 638-652.
  \doi{10.3758/s13428-016-0739-8}

Pessach, D. & Shmueli, E. (2022). A review on fairness in machine
  learning. \emph{ACM Computing Surveys}, 55(3), 1-44. \doi{10.1145/3494672}

Zafar, M. B., Valera, I., Gomez Rodriguez, M., & Gummadi, K. P.
  (2017). Fairness beyond disparate Ttreatment & disparate impact. In
  \emph{Proceedings of the 26th International Conference on World Wide Web}.
  \doi{10.1145/3038912.3052660}
}
\author{
Koen Derks, \email{k.derks@nyenrode.nl}
}
\keyword{algorithm}
\keyword{audit}
\keyword{bias}
\keyword{fairness}
\keyword{model}
\keyword{performance}
